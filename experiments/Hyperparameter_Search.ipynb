{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mJ7qTWjEwz-"
      },
      "source": [
        "# 0. Setup\n",
        "We will begin by importing all required libraries and fixing the plotting setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkBd8fm6ffZp",
        "outputId": "1dfd559a-f514-4c62-9017-d2398b97ee5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn[plot]\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn[plot])\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.7.1)\n",
            "Collecting datashader (from umap-learn[plot])\n",
            "  Downloading datashader-0.15.0-py2.py3-none-any.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (2.4.3)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.15.4)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.0.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.12.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.19.3)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn[plot]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn[plot]) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn[plot]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (3.1.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (23.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (4.6.3)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from colorcet->umap-learn[plot]) (0.5.0)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2022.12.1)\n",
            "Collecting datashape (from datashader->umap-learn[plot])\n",
            "  Downloading datashape-0.5.2.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (1.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.27.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (0.12.0)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2022.12.0)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (2.3.2)\n",
            "Requirement already satisfied: panel>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (2.1.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (3.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews->umap-learn[plot]) (6.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->umap-learn[plot]) (1.16.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (8.1.3)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (1.4.0)\n",
            "Requirement already satisfied: multipledispatch>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from datashape->datashader->umap-learn[plot]) (0.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.4)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask->datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=0.13.1->holoviews->umap-learn[plot]) (0.5.1)\n",
            "Building wheels for collected packages: pynndescent, umap-learn, datashape\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=9f812a0a2671a99fba6619319ab753b01e37207f2d385168875cc1235e36661b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=1dd2370ccdb75587598e0ee6a6007a3ea08a7e57486bcd9bf2f4bc8d9001bfba\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for datashape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datashape: filename=datashape-0.5.2-py3-none-any.whl size=59421 sha256=a009892a177dfc3487c79721fe2cd7709c8346c863cff69e586da8dcee6e068a\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/c6/63/a3c12ecc9fdea10a593271de5c56481b427ad4049b90a176e1\n",
            "Successfully built pynndescent umap-learn datashape\n",
            "Installing collected packages: datashape, pynndescent, umap-learn, datashader\n",
            "Successfully installed datashader-0.15.0 datashape-0.5.2 pynndescent-0.5.10 umap-learn-0.5.3\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.10/dist-packages (1.15.4)\n",
            "Requirement already satisfied: param<2.0,>=1.9.3 in /usr/local/lib/python3.10/dist-packages (from holoviews) (1.13.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from holoviews) (1.22.4)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.10/dist-packages (from holoviews) (2.3.2)\n",
            "Requirement already satisfied: panel>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from holoviews) (0.14.4)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from holoviews) (3.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from holoviews) (23.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from holoviews) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->holoviews) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->holoviews) (2022.7.1)\n",
            "Requirement already satisfied: bokeh<2.5.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (2.4.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (3.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (4.65.0)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (6.0.0)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from panel>=0.13.1->holoviews) (4.6.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh<2.5.0,>=2.4.0->panel>=0.13.1->holoviews) (3.1.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh<2.5.0,>=2.4.0->panel>=0.13.1->holoviews) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh<2.5.0,>=2.4.0->panel>=0.13.1->holoviews) (6.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh<2.5.0,>=2.4.0->panel>=0.13.1->holoviews) (6.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.20.0->holoviews) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=0.13.1->holoviews) (0.5.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.13.1->holoviews) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.13.1->holoviews) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.13.1->holoviews) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.13.1->holoviews) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh<2.5.0,>=2.4.0->panel>=0.13.1->holoviews) (2.1.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (5.5.6)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.23.3-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.3.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.5.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (23.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.1)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.7.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Installing collected packages: jedi, comm, ipykernel\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed comm-0.1.3 ipykernel-6.23.3 jedi-0.18.2\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.cluster import OPTICS\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn import mixture\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.spatial import distance\n",
        "#colab specific: ######################################################################\n",
        "!pip install umap-learn[plot]\n",
        "!pip install holoviews\n",
        "!pip install -U ipykernel\n",
        "#######################################################################################\n",
        "import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xkMplxyBfmpW"
      },
      "outputs": [],
      "source": [
        "#plot settings\n",
        "%matplotlib inline\n",
        "sns.set(style='white', context='notebook', rc={'figure.figsize':(3,3)})\n",
        "plt.ioff() #ensures that no plots are shown within the notebook if not explicity demanded by plt.show()\n",
        "\n",
        "#color configuration\n",
        "colors = list(mcolors.CSS4_COLORS.keys())\n",
        "colors = random.sample(colors, 100)\n",
        "colors = np.array(colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17PgBa48FzAE"
      },
      "source": [
        "# 1. Setting the Parameters\n",
        "For a better legibility of this document, we will store all parameter settings in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "64sjjIDGGB-7"
      },
      "outputs": [],
      "source": [
        "#Path to workspace\n",
        "w_path = '/content/gdrive/MyDrive/DIm_red_Clustering/'\n",
        "\n",
        "#Pre-processing of the imported data - choose between...\n",
        "# 'feature_stand': feature standardization leading to unit vairance and zero mean of all features across the samples\n",
        "# 'norm_vecs': Normalized embedding vectors, that project all embedding vectors on a unit sphere\n",
        "# 'none'\n",
        "pre_processing = 'norm_vecs'\n",
        "\n",
        "\n",
        "#Dimensionality reduction of the imported data - choose between...\n",
        "# 'PCA'\n",
        "# 'UMAP'\n",
        "dim_reduction = 'UMAP'\n",
        "\n",
        "\n",
        "#Data-generation network - choose between...\n",
        "# 'Mannheim':\n",
        "# 'adapters':\n",
        "data_generation = 'final'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqGMqlpOH0Qr"
      },
      "source": [
        "# 2. Loading the embeddings\n",
        "\n",
        "Now, we will have to load the user embeddings from the npy file they are stored in.\n",
        "\n",
        "We also want to check that the imported data has the desired dimensions, to make sure that nothing went wrong throughout the process of creating and storing the embeddings in the npy file, and importing them into this document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0MtN7iqH5U_",
        "outputId": "849bfced-637e-4d55-fbe7-47935bb35129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#colab specific import setup: ######################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "####################################################################################################\n",
        "\n",
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_final.npy'\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "print(path)\n",
        "\n",
        "#create folder to sort images into\n",
        "data_gen_path = w_path + data_generation\n",
        "if os.path.exists(data_gen_path) == False:\n",
        "  os.mkdir(data_gen_path)\n",
        "\n",
        "#Check that embeddings have the correct shape\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pvdqu4vI9Sn"
      },
      "source": [
        "# 3. Pre-processing the embeddings\n",
        "Before passing our user embeddings on to the dimensionality reduction, we will have to pre-process them, to make sure that not a few features only dominate the dimensionality reduction due to scale differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWfpfxuJfwA0"
      },
      "outputs": [],
      "source": [
        "#checking which means of data pre-processing to use\n",
        "if pre_processing == 'norm_vecs':\n",
        "  scale_factors = np.sum(data, axis = 1)\n",
        "  scale_factors = scale_factors[:, np.newaxis]\n",
        "  processed_data = data / scale_factors\n",
        "elif pre_processing =='feature_stand':\n",
        "  processed_data = StandardScaler().fit_transform(data)\n",
        "else:\n",
        "  processed_data = data\n",
        "\n",
        "#create folder to sort images into\n",
        "pre_processing_path = data_gen_path + '/' + pre_processing\n",
        "if os.path.exists(pre_processing_path) == False:\n",
        "  os.mkdir(pre_processing_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92wlMVhTK-I7"
      },
      "source": [
        "# 4. Performing Dimensionality Reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qSyZBX8dojj"
      },
      "outputs": [],
      "source": [
        "#initializing dict to save reduced dimensionality embeddings in, and to retrieve those embeddings based on the parameters used to create them\n",
        "embeddings = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLu151ClQsaw",
        "outputId": "216c894b-d2d6-4cd2-e0b5-2b006c111b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/DIm_red_Clustering/final/norm_vecs/UMAP/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Create folders to later deposit images in\n",
        "path_dimred = pre_processing_path + '/' + dim_reduction\n",
        "os.mkdir(path_dimred)\n",
        "path2D = path_dimred + '/' + str(2)\n",
        "os.mkdir(path2D)\n",
        "path3D = path_dimred + '/' + str(3)\n",
        "os.mkdir(path3D)\n",
        "\n",
        "#print(path2D)\n",
        "print(path3D)\n",
        "\n",
        "#Checking which means of dimensionality reduction to use\n",
        "if dim_reduction == 'UMAP':\n",
        "\n",
        "  #UMAP\n",
        "  #iterating over plausible hyperparameter values: Dimension, number of neighbors, minimum distance and distance metric\n",
        "  for n_dims in [3]:\n",
        "    for n_neighbors in np.concatenate((range(2,11,1), range(15,51,5))): #n_neighbors in steps of 1 from 2-10\n",
        "      for min_dist in range(0,10,1): #min_dist between 0 and 1 in steps of 0.1\n",
        "        for measure in ['euclidean','manhattan','cosine']: #different distance measures\n",
        "\n",
        "          reducer = umap.UMAP(n_components = n_dims, n_neighbors = n_neighbors, min_dist = min_dist/10, metric = measure) #initialize umap with desired hyperparams\n",
        "          reduced_data = reducer.fit_transform(processed_data)#calculate umap and return reduced embeddings\n",
        "          embeddings[data_generation+ pre_processing + dim_reduction + str(n_dims) + str(n_neighbors)+ str(min_dist/10) + measure] = reduced_data #saving embeddings for retrieval during clustering\n",
        "\n",
        "          if n_dims == 2:\n",
        "            #plotting result and saving plots as images\n",
        "            plt.figure()\n",
        "            plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1)\n",
        "            plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + str(n_neighbors)+ '     min_dist:' + str(min_dist/10) + '     metric:' + measure, fontsize=8)\n",
        "            path2D_image = path2D + '/' + str(n_neighbors) + '_' + str(min_dist/10) + '_' + measure +'.pdf'\n",
        "            plt.savefig(path2D_image, pad_inches = 15)\n",
        "\n",
        "          else:\n",
        "            #plotting result and saving plots as images\n",
        "            plt.figure()\n",
        "            ax = plt.axes(projection='3d')\n",
        "            ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2],s=0.1)\n",
        "            plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + str(n_neighbors)+ '     min_dist:' + str(min_dist/10) + '     metric:' + measure, fontsize=8)\n",
        "            path3D_image = path3D + '/' + str(n_neighbors) + '_' + str(min_dist/10) + '_' + measure +'.pdf'\n",
        "            plt.savefig(path3D_image, pad_inches = 15)\n",
        "\n",
        "    if os.path.exists(data_gen_path +'/' + 'reduced_embeddings') == False:\n",
        "      os.mkdir(data_gen_path +'/' +  'reduced_embeddings')\n",
        "\n",
        "    with open(data_gen_path + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_' + pre_processing + str(n_dims) + '.pickle', 'wb') as file:\n",
        "      pickle.dump(embeddings, file)\n",
        "\n",
        "\n",
        "else:\n",
        "  #PCA\n",
        "  #iterating over hyperparameter: Dimension\n",
        "  for n_dims in [2,3]:\n",
        "    pca = PCA(n_components = n_dims)\n",
        "    reduced_data = pca.fit_transform(processed_data)\n",
        "    embeddings[data_generation+ pre_processing + dim_reduction + str(n_dims) + 'na'+ 'na' + 'na'] = reduced_data #saving embeddings for retrieval during clustering\n",
        "\n",
        "    if n_dims == 2:\n",
        "      #plotting result and saving plots as images\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction, fontsize=8)\n",
        "      path2D_image = path2D + '/'+ 'PCA' + '.pdf'\n",
        "      plt.savefig(path2D_image, pad_inches = 15)\n",
        "\n",
        "    else:\n",
        "      #plotting result and saving plots as images\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2],s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction, fontsize=8)\n",
        "      path3D_image = path3D + '/' + 'PCA' +'.pdf'\n",
        "      plt.savefig(path3D_image, pad_inches = 15)\n",
        "\n",
        "    with open(data_gen_path + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_PCA' + pre_processing + '.pickle', 'wb') as file:\n",
        "      pickle.dump(embeddings, file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl1ZDb53R0S-"
      },
      "source": [
        "# 5. Selecting suitable reduced embeddings\n",
        "As a next step, we will have to visually inspect the resulting plots to determine the most suitable reduced dimensionality embeddings. We will note down the hyperparamters used to create these embeddings.\n",
        "\n",
        "The chosen embeddings can be retrieved later when clustering by listing the hyperparameters used to create them as a keyword for the \"*embeddings*\" dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ct7P_7WgWVg"
      },
      "outputs": [],
      "source": [
        "#extracting the saved embeddings from our files\n",
        "\n",
        "umap_feature_stand2 = {}\n",
        "umap_feature_stand3 = {}\n",
        "umap_norm_vecs2 = {}\n",
        "umap_norm_vecs3 = {}\n",
        "umap_none2 = {}\n",
        "umap_none3 = {}\n",
        "pca = {}\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_none2.pickle', 'rb') as file:\n",
        "    umap_none2 = pickle.load(file)\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_none3.pickle', 'rb') as file:\n",
        "    umap_none3 = pickle.load(file)\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_feature_stand2.pickle', 'rb') as file:\n",
        "    umap_feature_stand2 = pickle.load(file)\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_feature_stand3.pickle', 'rb') as file:\n",
        "    umap_feature_stand3 = pickle.load(file)\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_norm_vecs2.pickle', 'rb') as file:\n",
        "    umap_norm_vex2 = pickle.load(file)\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_UMAP_norm_vecs3.pickle', 'rb') as file:\n",
        "    umap_norm_vex3 = pickle.load(file)\n",
        "\n",
        "with open(w_path + data_generation + '/' + 'reduced_embeddings'+ '/' + 'red_embeddings_PCA.pickle', 'rb') as file:\n",
        "    pca = pickle.load(file)\n",
        "\n",
        "#Merging the saved embeddings into one dictionary\n",
        "embeddings = umap_none2 | umap_none3 | umap_feature_stand2 | umap_feature_stand3 | umap_norm_vex2 | umap_norm_vex3 | pca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZQk4bm5drOe"
      },
      "outputs": [],
      "source": [
        "#selecting best values from previous dimensionality reduction and placing them in iterable array\n",
        "\n",
        "data_gen_array = np.full((34,), 'final')\n",
        "\n",
        "pre_proc_array_1 = np.full((21,), 'feature_stand')\n",
        "pre_proc_array_2 = np.full((13,), 'norm_vecs')\n",
        "pre_proc_array = np.concatenate((pre_proc_array_1, pre_proc_array_2))\n",
        "\n",
        "dim_red_array = np.full((34,),'UMAP')\n",
        "\n",
        "n_dims_array_1 = np.full((10,),'2')\n",
        "n_dims_array_2 = np.full((11,), '3')\n",
        "n_dims_array_3 = np.full((7,),'2')\n",
        "n_dims_array_4 = np.full((6,), '3')\n",
        "n_dims_array = np.concatenate((n_dims_array_1, n_dims_array_2, n_dims_array_3, n_dims_array_4))\n",
        "\n",
        "neighbors_array = np.full((34,), '2')\n",
        "\n",
        "dist_array = ['0.4', '0.3', '0.6', '0.3', '0.7', '0.9', '0.3', '0.7', '0.5', '0.4', '0.4', '0.3', '0.6', '0.5', '0.6', '0.5', '0.6', '0.7','0.7','0.8','0.5','0.4','0.4','0.6','0.8','0.9','0.5','0.7','0.1', '0.2', '0.5', '0.6', '0.4','0.3']\n",
        "\n",
        "metric_array = ['cosine','cosine','cosine','euclidean','euclidean','euclidean','euclidean','manhattan','euclidean','euclidean','cosine','cosine','cosine','cosine','euclidean','manhattan','manhattan','euclidean','manhattan','euclidean','euclidean','euclidean','manhattan','euclidean','euclidean','euclidean','manhattan','euclidean','euclidean','manhattan','manhattan','manhattan','manhattan','euclidean']\n",
        "\n",
        "iter_array = np.column_stack((data_gen_array, pre_proc_array, dim_red_array, n_dims_array, neighbors_array, dist_array, metric_array))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl49ibMnMGjs"
      },
      "source": [
        "#6. Checking Dimensionality Reduction Validity\n",
        "We will proceed by validating the chosen means of dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz6p3uN3MSTw"
      },
      "source": [
        "##6.1 Sanity Check: Transforming Test Data to Lower Dimension\n",
        "We now want to investigate whether the learned dimensionality reduction performed on a train set of embeddings also projects the test set of embeddings to the existing clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uxrDjsoN6jM"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_testing2') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_testing')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.1, 0.05]:\n",
        "    for pre_processing in ['feature_stand','norm_vecs']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "      else:\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = umap_reducer.transform(scaled_test_data) #calculate umap and return reduced embeddings\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_testing' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWTR3qIKIsWz"
      },
      "source": [
        "While the results look quite nice in the case of vector normalization, they do not look appropriate when using feature_standardization.\n",
        "\n",
        "Our first thought was that maybe the test data influence the feature standardization's mean and std parameters more than previously assumed. However, when looking at the difference between the feature means and stds of the entire embeddings data set and the feature means and stds of just the train embeddings, the difference is marginal and hence negligible. This can thus not be the source of our problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2BJgi7_Zaww"
      },
      "outputs": [],
      "source": [
        "#Checking if sampling has a relevant impact on feature standardization\n",
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size= 0.1)\n",
        "\n",
        "for i in range(5):\n",
        "#checking which means of data pre-processing to use\n",
        "  mean = np.mean(train_data, axis = 0)\n",
        "  #mean = mean[:, np.newaxis]\n",
        "  std = np.std(train_data, axis = 0)\n",
        "  #std = std[:, np.newaxis]\n",
        "\n",
        "  total_mean = np.mean(data, axis = 0)\n",
        "  total_std = np.std(data,axis = 0)\n",
        "\n",
        "  d_mean = total_mean - mean\n",
        "  d_std = total_std - std\n",
        "\n",
        "  print(d_mean)\n",
        "  print('\\n')\n",
        "  print(d_std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qefNQVp3IEly"
      },
      "source": [
        "Next we will see that UMAP is not deterministic. If we perform UMAP twice with the same hyperparams and on the same data, the resulting plots will look slightly different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6qCLYtHjTI9"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_twoiterations') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_twoiterations')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.1, 0.01]:\n",
        "    for pre_processing in ['feature_stand','norm_vecs']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "\n",
        "        scale_factors = np.sum(data, axis = 1)\n",
        "        scale_factors = scale_factors[:, np.newaxis]\n",
        "        scaled_data = data / scale_factors\n",
        "      else:\n",
        "\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "        total_mean = np.mean(data, axis = 0)\n",
        "        total_std = np.std(data,axis = 0)\n",
        "        total_std = total_std[:, np.newaxis]\n",
        "        total_mean = total_mean[:, np.newaxis]\n",
        "        scaled_data = scaled_data = ((data.T - total_mean)/total_std).T\n",
        "\n",
        "\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_data1 = umap_reducer.fit_transform(scaled_data) #calculate umap and return reduced embeddings\n",
        "      #reduced_test_data = umap_reducer.transform(scaled_test_data) #calculate umap and return reduced embeddings\n",
        "      umap_total_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_data2 = umap_total_reducer.fit_transform(scaled_data)\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_data1[:, 0], reduced_data1[:, 1], s=0.1, color = 'silver')\n",
        "      #plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.scatter(reduced_data2[:, 0], reduced_data2[:, 1], s=0.1, color = 'olive')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_twoiterations' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.2' + '.pdf', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EduilR2qITeO"
      },
      "source": [
        "We want to see if maybe we can solve this issue by averaging over several UMAP results. This however doesn't always work, as can be seen in the resulting plots, and is thus not a reliable solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZTyK3zJgsqu"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_averaging') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_averaging')\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "\n",
        "embeddings_total = []\n",
        "embeddings_train = []\n",
        "\n",
        "for i in range(10):\n",
        "  for test_size in [0.1]:\n",
        "    for pre_processing in ['feature_stand']:\n",
        "\n",
        "\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "\n",
        "        scale_factors = np.sum(data, axis = 1)\n",
        "        scale_factors = scale_factors[:, np.newaxis]\n",
        "        scaled_data = data / scale_factors\n",
        "      else:\n",
        "        #sc = StandardScaler()\n",
        "        #sc.fit(train_data)\n",
        "        #scaled_train_data = sc.transform(train_data)\n",
        "        #scaled_test_data = sc.transform(test_data)\n",
        "        #X_orig= sc.inverse_tranform(X_scaled)\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        #mean_train = np.broadcast_to(mean, (mean.shape[0], train_data.shape[0]))\n",
        "        #print(mean_train.shape)\n",
        "        #mean_test = np.broadcast_to(mean, (mean.shape[0], test_data.shape[0]))\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "        total_mean = np.mean(data, axis = 0)\n",
        "        total_std = np.std(data,axis = 0)\n",
        "        total_std = total_std[:, np.newaxis]\n",
        "        total_mean = total_mean[:, np.newaxis]\n",
        "        scaled_data = scaled_data = ((data.T - total_mean)/total_std).T\n",
        "\n",
        "\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = umap_reducer.transform(scaled_test_data) #calculate umap and return reduced embeddings\n",
        "      umap_total_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_data = umap_total_reducer.fit_transform(scaled_data)\n",
        "      embeddings_total.append(reduced_data)\n",
        "      embeddings_train.append(reduced_train_data)\n",
        "\n",
        "\n",
        "\n",
        "#embeddings_total_avg = np.mean( np.array([ embeddings_total[0], embeddings_total[1], embeddings_total[2], embeddings_total[3],embeddings_total[4],embeddings_total[5],embeddings_total[6],embeddings_total[7],embeddings_total[8],embeddings_total[9],embeddings_total[10], embeddings_total[11], embeddings_total[12], embeddings_total[13],embeddings_total[14],embeddings_total[15],embeddings_total[16],embeddings_total[17],embeddings_total[18],embeddings_total[19] ]), axis=0 )\n",
        "#embeddings_train_avg = np.mean( np.array([ embeddings_train[0], embeddings_train[1], embeddings_train[2], embeddings_train[3],embeddings_train[4],embeddings_train[5],embeddings_train[6],embeddings_train[7],embeddings_train[8],embeddings_train[9],embeddings_train[10], embeddings_train[11], embeddings_train[12], embeddings_train[13],embeddings_train[14],embeddings_train[15],embeddings_train[16],embeddings_train[17],embeddings_train[18],embeddings_train[19] ]), axis=0 )#\n",
        "#embeddings_total_avg = np.mean( np.array([ embeddings_total[0], embeddings_total[1], embeddings_total[2], embeddings_total[3],embeddings_total[4] ]), axis=0 )\n",
        "#embeddings_train_avg = np.mean( np.array([ embeddings_train[0], embeddings_train[1], embeddings_train[2], embeddings_train[3],embeddings_train[4]]), axis=0 )\n",
        "embeddings_total_avg = np.mean( np.array([ embeddings_total[0], embeddings_total[1], embeddings_total[2], embeddings_total[3],embeddings_total[4],embeddings_total[5],embeddings_total[6],embeddings_total[7],embeddings_total[8],embeddings_total[9] ]), axis=0 )\n",
        "embeddings_train_avg = np.mean( np.array([ embeddings_train[0], embeddings_train[1], embeddings_train[2], embeddings_train[3],embeddings_train[4],embeddings_train[5],embeddings_train[6],embeddings_train[7],embeddings_train[8],embeddings_train[9] ]), axis=0 )\n",
        "plt.figure()\n",
        "plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "plt.scatter(embeddings_train_avg[:, 0], embeddings_train_avg[:, 1], s=0.1, color = 'silver')\n",
        "#plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "plt.scatter(embeddings_total_avg[:, 0], embeddings_total_avg[:, 1], s=0.1, color = 'olive')\n",
        "plt.savefig(w_path + '/' + 'UMAP_averaging' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.2' + '.pdf', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBYDLxq5nfpi"
      },
      "source": [
        "We'll now see how this works without doing any data pre-processing at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLtbMGAJnd1-"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_transform_wo_pp') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_transform_wo_pp')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.1, 0.05]:\n",
        "\n",
        "    train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "\n",
        "\n",
        "\n",
        "    umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "    reduced_train_data = umap_reducer.fit_transform(train_data) #calculate umap and return reduced embeddings\n",
        "    reduced_test_data = umap_reducer.transform(test_data) #calculate umap and return reduced embeddings\n",
        "    plt.figure()\n",
        "    plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "    plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "    plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "    plt.savefig(w_path + '/' + 'UMAP_transform_wo_pp' + '/' + 'test_set_size' + str(int(test_size*100)) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuK5qI7NOt87"
      },
      "source": [
        "Finally, we will try a different approach. We will determine the k nearest neighbors of the test data embedding and interpolate between their lower-dim embeddings to calculate the test data embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1xW0p3frW-7"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_cosine_interpolation') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_cosine_interpolation')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.1, 0.05]:\n",
        "    for pre_processing in ['feature_stand']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "      nn = NearestNeighbors(n_neighbors=4)\n",
        "      neighbors = np.empty((test_data.shape[0],3))\n",
        "      for a in range(test_data.shape[0]):\n",
        "        test_data_item = test_data[a]\n",
        "        test_data_item = test_data_item[np.newaxis,:]\n",
        "        #train_data_transposed = train_data\n",
        "\n",
        "        total_data = np.concatenate((train_data, test_data_item), axis = 0)\n",
        "\n",
        "        nn.fit(total_data)\n",
        "        _ , neighbor = nn.kneighbors(test_data_item)\n",
        "\n",
        "\n",
        "        neighbors[a][:] = neighbor[0][1:4]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "      else:\n",
        "        #sc = StandardScaler()\n",
        "        #sc.fit(train_data)\n",
        "        #scaled_train_data = sc.transform(train_data)\n",
        "        #scaled_test_data = sc.transform(test_data)\n",
        "        #X_orig= sc.inverse_tranform(X_scaled)\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        #mean_train = np.broadcast_to(mean, (mean.shape[0], train_data.shape[0]))\n",
        "        #print(mean_train.shape)\n",
        "        #mean_test = np.broadcast_to(mean, (mean.shape[0], test_data.shape[0]))\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "      print(test_data.shape)\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'cosine') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = np.empty((test_data.shape[0], reduced_train_data.shape[1]))\n",
        "      for j in range(test_data.shape[0]):\n",
        "        #print(neighbors[j][0])\n",
        "        #print(reduced_train_data[int(neighbors[j][0])])\n",
        "        reduced_test_data[j] = np.mean( np.array([reduced_train_data[int(neighbors[j][0])], reduced_train_data[int(neighbors[j][1])], reduced_train_data[int(neighbors[j][2])]]), axis = 0)\n",
        "\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_cosine_interpolation' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REgcIvs8O_my"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_interpolation') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_interpolation')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.1, 0.05]:\n",
        "    for pre_processing in ['feature_stand','norm_vecs']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "      nn = NearestNeighbors(n_neighbors=4)\n",
        "      neighbors = np.empty((test_data.shape[0],3))\n",
        "      for a in range(test_data.shape[0]):\n",
        "        test_data_item = test_data[a]\n",
        "        test_data_item = test_data_item[np.newaxis,:]\n",
        "        #train_data_transposed = train_data\n",
        "\n",
        "        total_data = np.concatenate((train_data, test_data_item), axis = 0)\n",
        "\n",
        "        nn.fit(total_data)\n",
        "        _ , neighbor = nn.kneighbors(test_data_item)\n",
        "\n",
        "\n",
        "        neighbors[a][:] = neighbor[0][1:4]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "      else:\n",
        "        #sc = StandardScaler()\n",
        "        #sc.fit(train_data)\n",
        "        #scaled_train_data = sc.transform(train_data)\n",
        "        #scaled_test_data = sc.transform(test_data)\n",
        "        #X_orig= sc.inverse_tranform(X_scaled)\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        #mean_train = np.broadcast_to(mean, (mean.shape[0], train_data.shape[0]))\n",
        "        #print(mean_train.shape)\n",
        "        #mean_test = np.broadcast_to(mean, (mean.shape[0], test_data.shape[0]))\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "      print(test_data.shape)\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = np.empty((test_data.shape[0], reduced_train_data.shape[1]))\n",
        "      for j in range(test_data.shape[0]):\n",
        "        #print(neighbors[j][0])\n",
        "        #print(reduced_train_data[int(neighbors[j][0])])\n",
        "        reduced_test_data[j] = np.mean( np.array([reduced_train_data[int(neighbors[j][0])], reduced_train_data[int(neighbors[j][1])], reduced_train_data[int(neighbors[j][2])]]), axis = 0)\n",
        "\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_interpolation' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkDSBRBMsabS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1IZGvw7sdsH"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_cosine_afterpreprocessing') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_cosine_afterpreprocessing')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.3, 0.1, 0.05]:\n",
        "    for pre_processing in ['feature_stand']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "      nn = NearestNeighbors(n_neighbors=4)\n",
        "      neighbors = np.empty((test_data.shape[0],3))\n",
        "\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "      else:\n",
        "        #sc = StandardScaler()\n",
        "        #sc.fit(train_data)\n",
        "        #scaled_train_data = sc.transform(train_data)\n",
        "        #scaled_test_data = sc.transform(test_data)\n",
        "        #X_orig= sc.inverse_tranform(X_scaled)\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        #mean_train = np.broadcast_to(mean, (mean.shape[0], train_data.shape[0]))\n",
        "        #print(mean_train.shape)\n",
        "        #mean_test = np.broadcast_to(mean, (mean.shape[0], test_data.shape[0]))\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "      for a in range(test_data.shape[0]):\n",
        "        test_data_item = scaled_test_data[a]\n",
        "        test_data_item = test_data_item[np.newaxis,:]\n",
        "        #train_data_transposed = train_data\n",
        "\n",
        "        total_data = np.concatenate((scaled_train_data, test_data_item), axis = 0)\n",
        "\n",
        "        nn.fit(total_data)\n",
        "        _ , neighbor = nn.kneighbors(test_data_item)\n",
        "\n",
        "\n",
        "        neighbors[a][:] = neighbor[0][1:4]\n",
        "\n",
        "      print(test_data.shape)\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'cosine') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = np.empty((test_data.shape[0], reduced_train_data.shape[1]))\n",
        "      for j in range(test_data.shape[0]):\n",
        "        #print(neighbors[j][0])\n",
        "        #print(reduced_train_data[int(neighbors[j][0])])\n",
        "        reduced_test_data[j] = np.mean( np.array([reduced_train_data[int(neighbors[j][0])], reduced_train_data[int(neighbors[j][1])], reduced_train_data[int(neighbors[j][2])]]), axis = 0)\n",
        "\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_cosine_afterpreprocessing' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv5iCgNMflFG",
        "outputId": "2a4e3558-4dde-4074-cc75-2070e89760bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(619, 768)\n",
            "(619, 768)\n",
            "(207, 768)\n",
            "(207, 768)\n",
            "(104, 768)\n",
            "(104, 768)\n",
            "(619, 768)\n",
            "(619, 768)\n",
            "(207, 768)\n",
            "(207, 768)\n",
            "(104, 768)\n",
            "(104, 768)\n",
            "(619, 768)\n",
            "(619, 768)\n",
            "(207, 768)\n",
            "(207, 768)\n",
            "(104, 768)\n",
            "(104, 768)\n",
            "(619, 768)\n",
            "(619, 768)\n",
            "(207, 768)\n",
            "(207, 768)\n",
            "(104, 768)\n",
            "(104, 768)\n",
            "(619, 768)\n",
            "(619, 768)\n",
            "(207, 768)\n",
            "(207, 768)\n",
            "(104, 768)\n",
            "(104, 768)\n"
          ]
        }
      ],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_nn_after_preprocessing') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_nn_after_preprocessing')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.3, 0.1, 0.05]:\n",
        "    for pre_processing in ['feature_stand','norm_vecs']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "      nn = NearestNeighbors(n_neighbors=4)\n",
        "      neighbors = np.empty((test_data.shape[0],3))\n",
        "\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "      else:\n",
        "        #sc = StandardScaler()\n",
        "        #sc.fit(train_data)\n",
        "        #scaled_train_data = sc.transform(train_data)\n",
        "        #scaled_test_data = sc.transform(test_data)\n",
        "        #X_orig= sc.inverse_tranform(X_scaled)\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        #mean_train = np.broadcast_to(mean, (mean.shape[0], train_data.shape[0]))\n",
        "        #print(mean_train.shape)\n",
        "        #mean_test = np.broadcast_to(mean, (mean.shape[0], test_data.shape[0]))\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "      for a in range(test_data.shape[0]):\n",
        "        test_data_item = scaled_test_data[a]\n",
        "        test_data_item = test_data_item[np.newaxis,:]\n",
        "        #train_data_transposed = train_data\n",
        "\n",
        "        total_data = np.concatenate((scaled_train_data, test_data_item), axis = 0)\n",
        "\n",
        "        nn.fit(total_data)\n",
        "        _ , neighbor = nn.kneighbors(test_data_item)\n",
        "\n",
        "\n",
        "        neighbors[a][:] = neighbor[0][1:4]\n",
        "\n",
        "      print(test_data.shape)\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = np.empty((test_data.shape[0], reduced_train_data.shape[1]))\n",
        "      for j in range(test_data.shape[0]):\n",
        "        #print(neighbors[j][0])\n",
        "        #print(reduced_train_data[int(neighbors[j][0])])\n",
        "        reduced_test_data[j] = np.mean( np.array([reduced_train_data[int(neighbors[j][0])], reduced_train_data[int(neighbors[j][1])], reduced_train_data[int(neighbors[j][2])]]), axis = 0)\n",
        "\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_nn_after_preprocessing' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1izRwF-nctY0"
      },
      "outputs": [],
      "source": [
        "#checking which data to import\n",
        "if data_generation == 'adapters':\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_adapters.npy'\n",
        "else:\n",
        "  path='/content/gdrive/MyDrive/DIm_red_Clustering/user_embeddings_32d.npy'\n",
        "\n",
        "\n",
        "#loading the data\n",
        "data = np.load(path)\n",
        "\n",
        "if os.path.exists(w_path +'/' + 'UMAP_nearestneighbor') == False:\n",
        "  os.mkdir(w_path +'/' +  'UMAP_nearestneighbor')\n",
        "\n",
        "for i in range(5):\n",
        "  for test_size in [0.3, 0.1, 0.05]:\n",
        "    for pre_processing in ['feature_stand','norm_vecs']:\n",
        "\n",
        "      train_data, test_data = train_test_split(data, test_size= test_size)\n",
        "      nn = NearestNeighbors(n_neighbors=2)\n",
        "      neighbors = np.empty((test_data.shape[0],1))\n",
        "      for a in range(test_data.shape[0]):\n",
        "        test_data_item = test_data[a]\n",
        "        test_data_item = test_data_item[np.newaxis,:]\n",
        "        #train_data_transposed = train_data\n",
        "\n",
        "        total_data = np.concatenate((train_data, test_data_item), axis = 0)\n",
        "\n",
        "        nn.fit(total_data)\n",
        "        _ , neighbor = nn.kneighbors(test_data_item)\n",
        "\n",
        "\n",
        "        neighbors[a][:] = neighbor[0][1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #checking which means of data pre-processing to use\n",
        "      if pre_processing == 'norm_vecs':\n",
        "        scale_factors_train = np.sum(train_data, axis = 1)\n",
        "        scale_factors_train = scale_factors_train[:, np.newaxis]\n",
        "        scaled_train_data = train_data / scale_factors_train\n",
        "        scale_factors_test = np.sum(test_data, axis = 1)\n",
        "        scale_factors_test = scale_factors_test[:, np.newaxis]\n",
        "        scaled_test_data = test_data / scale_factors_test\n",
        "      else:\n",
        "        #sc = StandardScaler()\n",
        "        #sc.fit(train_data)\n",
        "        #scaled_train_data = sc.transform(train_data)\n",
        "        #scaled_test_data = sc.transform(test_data)\n",
        "        #X_orig= sc.inverse_tranform(X_scaled)\n",
        "        mean = np.mean(train_data, axis = 0)\n",
        "        mean = mean[:, np.newaxis]\n",
        "        #mean_train = np.broadcast_to(mean, (mean.shape[0], train_data.shape[0]))\n",
        "        #print(mean_train.shape)\n",
        "        #mean_test = np.broadcast_to(mean, (mean.shape[0], test_data.shape[0]))\n",
        "        std = np.std(train_data, axis = 0)\n",
        "        std = std[:, np.newaxis]\n",
        "        scaled_train_data = ((train_data.T - mean)/std).T\n",
        "        scaled_test_data = ((test_data.T - mean)/std).T\n",
        "        mean = np.mean(scaled_train_data, axis = 0)\n",
        "        var = np.var(scaled_train_data, axis = 0)\n",
        "\n",
        "\n",
        "      umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.3, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "      reduced_train_data = umap_reducer.fit_transform(scaled_train_data) #calculate umap and return reduced embeddings\n",
        "      reduced_test_data = np.empty((test_data.shape[0], reduced_train_data.shape[1]))\n",
        "      for j in range(test_data.shape[0]):\n",
        "        #print(neighbors[j][0])\n",
        "        #print(reduced_train_data[int(neighbors[j][0])])\n",
        "        reduced_test_data[j] = reduced_train_data[int(neighbors[j])]\n",
        "\n",
        "      plt.figure()\n",
        "      plt.title(pre_processing + '   test_set_size:' + str(test_size*100) +'%' + '   nr:' + str(i+1) )\n",
        "      plt.scatter(reduced_train_data[:, 0], reduced_train_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_test_data[:, 0], reduced_test_data[:, 1], s=0.1, color = 'crimson')\n",
        "      plt.savefig(w_path + '/' + 'UMAP_nearestneighbor' + '/' + pre_processing + 'test_set_size' + str(test_size*100) +'%' + '_nr' + str(i+1) + '.pdf', bbox_inches='tight')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqshf9QuJkma"
      },
      "source": [
        "##6.2 Sanity Check: Proximity Preservation\n",
        "In this section, we want to see if the proximity of two points in the higher-dimensional embedding is preserved in the lower-dimensional embedding.\n",
        "\n",
        "In order to do this, we will perform three checks:\n",
        "\n",
        "**Check 1:** Does the single nearest neighbor of a point in the high-dimensional embedding lie within the same cluster as this point in the low-dimensional embedding?\n",
        "\n",
        "**Check 2:** Do the three nearest neighbors of a point in the high-dimensional embedding lie within the same cluster as this point in the low-dimensional embedding?\n",
        "\n",
        "**Check 3:** Does a point that is not in close proximity of another point in the high-dimensional embedding lie in a different cluster than this point in the low-dimensional embedding?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD713RzD5h4o"
      },
      "outputs": [],
      "source": [
        "#Prepare folders to store resulting plots in\n",
        "\n",
        "sanity_check_path = w_path + 'UMAP_sanity_checks'\n",
        "check_1_path = sanity_check_path + '/' + 'check1'\n",
        "check_2_path = sanity_check_path + '/' + 'check2'\n",
        "check_3_path = sanity_check_path + '/' + 'check3'\n",
        "\n",
        "if os.path.exists(sanity_check_path) == False:\n",
        "  os.mkdir(sanity_check_path)\n",
        "if os.path.exists(check_1_path) == False:\n",
        "  os.mkdir(check_1_path)\n",
        "if os.path.exists(check_2_path) == False:\n",
        "  os.mkdir(check_2_path)\n",
        "if os.path.exists(check_3_path) == False:\n",
        "  os.mkdir(check_3_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ax56mR38tK"
      },
      "source": [
        "### 6.2.1 Check 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZK1kR_n4vFJ"
      },
      "outputs": [],
      "source": [
        "color_iter = ['crimson','olive','cyan','gold']\n",
        "nr_embeddings = data.shape[0]\n",
        "idx = np.random.randint(nr_embeddings, size=4)\n",
        "neighbors = np.empty((4,2))\n",
        "\n",
        "for pre_processing in ['none', 'feature_stand', 'norm_vecs']:\n",
        "  if pre_processing == 'norm_vecs':\n",
        "    scale_factors = np.sum(data, axis = 1)\n",
        "    scale_factors = scale_factors[:, np.newaxis]\n",
        "    processed_data = data / scale_factors\n",
        "  elif pre_processing =='feature_stand':\n",
        "    processed_data = StandardScaler().fit_transform(data)\n",
        "  else:\n",
        "    processed_data = data\n",
        "  for measure in ['euclidean', 'cosine','manhattan']: #iteration over different reduced dim embeddings\n",
        "    for i in range(4):\n",
        "      sample_highdim = data[idx[i]]\n",
        "      nn = NearestNeighbors(n_neighbors=2, metric = measure)\n",
        "      nn.fit(data)\n",
        "      sample_highdim = data[idx[i]]\n",
        "      sample_highdim = sample_highdim[np.newaxis,:]\n",
        "      _ , neighbor = nn.kneighbors(sample_highdim)\n",
        "      neighbors[i] = neighbor\n",
        "    neighbors = np.asarray(neighbors, dtype = 'int')\n",
        "    #samples = data[idx,:]\n",
        "    for n_neighbors in [2, 5,10,30]:\n",
        "      for min_dist in [0.2, 0.4]:\n",
        "\n",
        "        umap_reducer = umap.UMAP(n_components = 2, n_neighbors = n_neighbors, min_dist = min_dist, metric = measure) #initialize umap with desired hyperparams\n",
        "        reduced_data = umap_reducer.fit_transform(processed_data) #calculate umap and return reduced embeddings\n",
        "        plt.figure()\n",
        "        plt.title(str(n_neighbors) +'   ' + str(min_dist) +'   ' + measure + '\\n' + pre_processing)\n",
        "        plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, color = 'silver')\n",
        "        for i in range(4):\n",
        "          #sample = reduced_data[idx[i]]\n",
        "          #sample = sample[np.newaxis,:]\n",
        "          neighbor_embeddings = reduced_data[neighbors[i],:]\n",
        "          color = color_iter[i]\n",
        "          plt.scatter(neighbor_embeddings[:, 0], neighbor_embeddings[:, 1], s=0.3, color = color)\n",
        "        plt.savefig(check_1_path + '/' + pre_processing + '_' + measure + '_' + str(n_neighbors) +'_' + str(min_dist) + '.pdf', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU5rbG566zib"
      },
      "source": [
        "### 6.2.2 Check 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUwOTtEW60Oi",
        "outputId": "1f6b6931-3532-46da-c158-6e7588923c7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Trying for high n_neighbor hyperparameter\n",
        "\n",
        "color_iter = ['crimson','olive','cyan','gold']\n",
        "nr_embeddings = data.shape[0]\n",
        "idx = np.random.randint(nr_embeddings, size=4)\n",
        "neighbors = np.empty((4,20))\n",
        "\n",
        "for pre_processing in ['none', 'feature_stand', 'norm_vecs']:\n",
        "  if pre_processing == 'norm_vecs':\n",
        "    scale_factors = np.sum(data, axis = 1)\n",
        "    scale_factors = scale_factors[:, np.newaxis]\n",
        "    processed_data = data / scale_factors\n",
        "  elif pre_processing =='feature_stand':\n",
        "    processed_data = StandardScaler().fit_transform(data)\n",
        "  else:\n",
        "    processed_data = data\n",
        "  for measure in ['euclidean', 'cosine','manhattan']: #iteration over different reduced dim embeddings\n",
        "    for i in range(4):\n",
        "      sample_highdim = data[idx[i]]\n",
        "      nn = NearestNeighbors(n_neighbors=20, metric = measure)\n",
        "      nn.fit(data)\n",
        "      sample_highdim = data[idx[i]]\n",
        "      sample_highdim = sample_highdim[np.newaxis,:]\n",
        "      _ , neighbor = nn.kneighbors(sample_highdim)\n",
        "      neighbors[i] = neighbor\n",
        "    neighbors = np.asarray(neighbors, dtype = 'int')\n",
        "    #samples = data[idx,:]\n",
        "    for n_neighbors in [2, 5,10,30]:\n",
        "      for min_dist in [0.2, 0.4]:\n",
        "\n",
        "        umap_reducer = umap.UMAP(n_components = 2, n_neighbors = n_neighbors, min_dist = min_dist, metric = measure) #initialize umap with desired hyperparams\n",
        "        reduced_data = umap_reducer.fit_transform(processed_data) #calculate umap and return reduced embeddings\n",
        "        plt.figure()\n",
        "        plt.title(str(n_neighbors) +'   ' + str(min_dist) +'   ' + measure + '\\n' + pre_processing)\n",
        "        plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, color = 'silver')\n",
        "        for i in range(4):\n",
        "          #sample = reduced_data[idx[i]]\n",
        "          #sample = sample[np.newaxis,:]\n",
        "          neighbor_embeddings = reduced_data[neighbors[i],:]\n",
        "          color = color_iter[i]\n",
        "          plt.scatter(neighbor_embeddings[:, 0], neighbor_embeddings[:, 1], s=0.3, color = color)\n",
        "        plt.savefig(check_2_path + '/' + '20points' + '_' + pre_processing + '_' + measure + '_' + str(n_neighbors) +'_' + str(min_dist) + '.pdf', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj5DfEIbn4Q4"
      },
      "source": [
        "### 6.2.3 Check 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKYQ5InKoDZO",
        "outputId": "41edbe34-43ec-4467-bd88-2f36067ff972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "euclideaneuclidean\n",
            "28.872692\n",
            "manhattan39.003998\n",
            "cosine1.732623815536499\n",
            "28.889385\n",
            "manhattan39.022457\n",
            "cosine1.7324561476707458\n",
            "28.855011\n",
            "manhattan39.031185\n",
            "cosine1.7258954644203186\n",
            "28.893576\n",
            "manhattan39.037758\n",
            "cosine1.7324785590171814\n",
            "cosinecosine\n",
            "19.590925\n",
            "manhattan24.320436\n",
            "cosine1.4616790413856506\n",
            "19.85264\n",
            "manhattan24.470734\n",
            "cosine1.5047436952590942\n",
            "19.502613\n",
            "manhattan23.74619\n",
            "cosine1.5149433612823486\n",
            "20.14878\n",
            "manhattan22.343721\n",
            "cosine1.8144596815109253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "manhattancityblock\n",
            "17.416327\n",
            "manhattan22.863401\n",
            "cosine1.9120588898658752\n",
            "24.273214\n",
            "manhattan32.5449\n",
            "cosine1.1974610388278961\n",
            "17.373507\n",
            "manhattan22.83827\n",
            "cosine1.9056857824325562\n",
            "17.14573\n",
            "manhattan22.471436\n",
            "cosine1.9089009761810303\n",
            "euclideaneuclidean\n",
            "8.085731\n",
            "manhattan10.304594\n",
            "cosine0.12791383266448975\n",
            "8.051851\n",
            "manhattan10.244055\n",
            "cosine0.12796783447265625\n",
            "8.047795\n",
            "manhattan10.2138\n",
            "cosine0.12751871347427368\n",
            "7.8789754\n",
            "manhattan9.927461\n",
            "cosine0.12224125862121582\n",
            "cosinecosine\n",
            "15.4954\n",
            "manhattan20.543406\n",
            "cosine0.9266278371214867\n",
            "18.883387\n",
            "manhattan22.175663\n",
            "cosine1.1907411515712738\n",
            "18.655613\n",
            "manhattan21.624691\n",
            "cosine1.2083291858434677\n",
            "18.243114\n",
            "manhattan20.253387\n",
            "cosine1.2636833488941193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "manhattancityblock\n",
            "27.096205\n",
            "manhattan28.689184\n",
            "cosine1.8188676238059998\n",
            "21.473225\n",
            "manhattan28.610256\n",
            "cosine0.9060535132884979\n",
            "27.113916\n",
            "manhattan28.693752\n",
            "cosine1.8183984756469727\n",
            "27.11726\n",
            "manhattan28.692442\n",
            "cosine1.819487452507019\n",
            "euclideaneuclidean\n",
            "5.2437997\n",
            "manhattan5.8041344\n",
            "cosine0.4315829873085022\n",
            "14.718917\n",
            "manhattan18.966797\n",
            "cosine1.0501969009637833\n",
            "20.629534\n",
            "manhattan25.012136\n",
            "cosine1.8915109634399414\n",
            "14.779425\n",
            "manhattan18.305595\n",
            "cosine0.8390147387981415\n",
            "cosinecosine\n",
            "0.14867732\n",
            "manhattan0.20231593\n",
            "cosine2.9325485229492188e-05\n",
            "0.5890975\n",
            "manhattan0.8185798\n",
            "cosine0.0008529424667358398\n",
            "0.65066296\n",
            "manhattan0.82714266\n",
            "cosine0.00025600194931030273\n",
            "3.920839\n",
            "manhattan5.2097654\n",
            "cosine0.10112309455871582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "manhattancityblock\n",
            "9.1292925\n",
            "manhattan11.49642\n",
            "cosine0.5921822786331177\n",
            "22.860207\n",
            "manhattan32.328575\n",
            "cosine1.98752760887146\n",
            "21.906595\n",
            "manhattan30.585163\n",
            "cosine1.9268772602081299\n",
            "23.075054\n",
            "manhattan23.465057\n",
            "cosine1.7175220251083374\n"
          ]
        }
      ],
      "source": [
        "color_iter = ['crimson','olive','cyan','gold']\n",
        "for pre_processing in ['none', 'feature_stand', 'norm_vecs']:\n",
        "\n",
        "  if pre_processing == 'norm_vecs':\n",
        "    scale_factors = np.sum(data, axis = 1)\n",
        "    scale_factors = scale_factors[:, np.newaxis]\n",
        "    processed_data = data / scale_factors\n",
        "  elif pre_processing =='feature_stand':\n",
        "    processed_data = StandardScaler().fit_transform(data)\n",
        "  else:\n",
        "    processed_data = data\n",
        "\n",
        "  for metric in ['euclidean', 'cosine', 'manhattan']:\n",
        "    n_neighbors = 2\n",
        "    min_dist = 0.4\n",
        "    umap_reducer = umap.UMAP(n_components = 2, n_neighbors = n_neighbors, min_dist = min_dist, metric = metric) #initialize umap with desired hyperparams\n",
        "    reduced_data = umap_reducer.fit_transform(processed_data) #calculate umap and return reduced embeddings\n",
        "\n",
        "    if metric == 'manhattan':\n",
        "      metric_c = 'cityblock'\n",
        "    else:\n",
        "      metric_c = metric\n",
        "    print(metric + metric_c)\n",
        "    distance_vec = pdist(data, metric = metric_c)\n",
        "    distance_matrix = squareform(distance_vec)\n",
        "    for testpoints in range(4):\n",
        "      max_distance, [i,j] = np.nanmax(distance_matrix), np.unravel_index( distance_matrix.argmax(), distance_matrix.shape )\n",
        "      #print(np.linalg.norm(data[i] - data[j]) - max_distance)\n",
        "      #print('manhattan' + str(distance.cityblock(data[i], data[j]) - max_distance))\n",
        "      #print('cosine'+ str(distance.cosine(data[i], data[j]) - max_distance))\n",
        "      #print(distance_matrix[i,j])\n",
        "      #print(i)\n",
        "      #print(j)\n",
        "      distance_matrix[i,j] = 0\n",
        "      distance_matrix[j,i] = 0\n",
        "      color = color_iter[testpoints]\n",
        "      plt.figure()\n",
        "      plt.title('Distant Points' + '\\n' + pre_processing + str(n_neighbors) +'   ' + str(min_dist) +'   ' + metric )\n",
        "      plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, color = 'silver')\n",
        "      plt.scatter(reduced_data[i, 0], reduced_data[i, 1], s=0.3, color = color)\n",
        "      plt.scatter(reduced_data[j, 0], reduced_data[j, 1], s=0.3, color = color)\n",
        "      print(np.linalg.norm(reduced_data[i] - reduced_data[j]) )\n",
        "      print('manhattan' + str(distance.cityblock(reduced_data[i], reduced_data[j])))\n",
        "      print('cosine'+ str(distance.cosine(reduced_data[i], reduced_data[j])))\n",
        "      plt.savefig(check_3_path + '/' + pre_processing + '_' + metric + '_' + str(n_neighbors) +'_' + str(min_dist) + '.pdf', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGPC8QVGA-_n",
        "outputId": "562e9667-eb99-47cf-dc36-158718f2316d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "29.953537422259267\n",
            "6.72263167680873\n",
            "(1, 2)\n",
            "4.5948486328125\n",
            "\n",
            "\n",
            "22.94293859250076\n",
            "6.904077559438377\n",
            "\n",
            "\n",
            "17.136880389018163\n",
            "6.333739329654997\n",
            "\n",
            "\n",
            "13.245203463214919\n",
            "5.725724270397775\n"
          ]
        }
      ],
      "source": [
        "n_neighbors = 2\n",
        "min_dist = 0.4\n",
        "umap_reducer = umap.UMAP(n_components = 2, n_neighbors = 2, min_dist = 0.4, metric = 'euclidean') #initialize umap with desired hyperparams\n",
        "reduced_data = umap_reducer.fit_transform(processed_data) #calculate umap and return reduced embeddings\n",
        "dbscan = DBSCAN(eps= 3/2, min_samples = 50)\n",
        "dbscan.fit(reduced_data)\n",
        "labels = dbscan.labels_\n",
        "idx_c1 = [i for i in range(len(labels)) if labels[i] == 1]\n",
        "c1_emb = [data[idx_c1[i]] for i in range(len(idx_c1))]\n",
        "idx_c2 = [i for i in range(len(labels)) if labels[i] == 2]\n",
        "c2_emb = [data[idx_c2[i]] for i in range(len(idx_c2))]\n",
        "idx_c3 = [i for i in range(len(labels)) if labels[i] == 3]\n",
        "c3_emb = [data[idx_c3[i]] for i in range(len(idx_c3))]\n",
        "\n",
        "print(idx_c1[0] in idx_c2)\n",
        "print(idx_c1[0] in idx_c3)\n",
        "print(idx_c1[10] in idx_c2)\n",
        "print(idx_c1[10] in idx_c3)\n",
        "print(idx_c1[15] in idx_c2)\n",
        "print(idx_c1[15] in idx_c3)\n",
        "\n",
        "distance_vec = pdist(data)\n",
        "distance_matrix = squareform(distance_vec)\n",
        "max_distance, median, [i,j] = np.nanmax(distance_matrix), np.median(distance_matrix),np.unravel_index( distance_matrix.argmax(), distance_matrix.shape )\n",
        "print(max_distance)\n",
        "print(median)\n",
        "\n",
        "neighbors = np.empty((1,2))\n",
        "idx = np.random.randint(data.shape[0])\n",
        "sample_highdim = data[idx]\n",
        "nn = NearestNeighbors(n_neighbors=2, metric = 'euclidean')\n",
        "nn.fit(data)\n",
        "sample_highdim_a = sample_highdim[np.newaxis,:]\n",
        "_ , neighbors_array = nn.kneighbors(sample_highdim_a)\n",
        "print(neighbors_array.shape)\n",
        "neighbor = neighbors_array[0][1]\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor])\n",
        "print(min_distance)\n",
        "\n",
        "distance_vec = pdist(c1_emb)\n",
        "distance_matrix = squareform(distance_vec)\n",
        "c1_max_distance, c1_median, [c1_i,c1_j] = np.nanmax(distance_matrix), np.median(distance_matrix),np.unravel_index( distance_matrix.argmax(), distance_matrix.shape )\n",
        "print('\\n')\n",
        "print(c1_max_distance)\n",
        "print(c1_median)\n",
        "\n",
        "distance_vec = pdist(c2_emb)\n",
        "distance_matrix = squareform(distance_vec)\n",
        "c2_max_distance, c2_median, [c2_i,c2_j] = np.nanmax(distance_matrix), np.median(distance_matrix), np.unravel_index( distance_matrix.argmax(), distance_matrix.shape )\n",
        "print('\\n')\n",
        "print(c2_max_distance)\n",
        "print(c2_median)\n",
        "\n",
        "distance_vec = pdist(c3_emb)\n",
        "distance_matrix = squareform(distance_vec)\n",
        "c3_max_distance, c3_median, [c3_i,c3_j] = np.nanmax(distance_matrix),np.median(distance_matrix), np.unravel_index( distance_matrix.argmax(), distance_matrix.shape )\n",
        "print('\\n')\n",
        "print(c3_max_distance)\n",
        "print(c3_median)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxSlWQK0L4EI",
        "outputId": "741eccf2-c969-4ef2-eaf5-bf59854a97cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.248859405517578\n",
            "4.33917760848999\n",
            "4.3432722091674805\n",
            "4.383040428161621\n",
            "4.47647762298584\n",
            "4.531210899353027\n",
            "4.532975673675537\n",
            "4.560563564300537\n"
          ]
        }
      ],
      "source": [
        "neighbors = np.empty((1,10))\n",
        "idx = np.random.randint(data.shape[0])\n",
        "sample_highdim = data[idx]\n",
        "nn = NearestNeighbors(n_neighbors=10, metric = 'euclidean')\n",
        "nn.fit(data)\n",
        "sample_highdim_a = sample_highdim[np.newaxis,:]\n",
        "_ , neighbors_array = nn.kneighbors(sample_highdim_a)\n",
        "neighbor = neighbors_array[0][1:9]\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[0]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[1]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[2]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[3]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[4]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[5]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[6]])\n",
        "print(min_distance)\n",
        "min_distance = distance.euclidean(sample_highdim, data[neighbor[7]])\n",
        "print(min_distance)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpluG6GOSwaH"
      },
      "source": [
        "# 7. Clustering\n",
        "Finally, we will have to cluster the reduced dimensionality embeddings.\n",
        "\n",
        "In this section, we will try out different clustering algorithms with different hyperparameters each. We will furthermore use the xxx metric to support qualitative assesments of the clustering, enabling us to choose our final set of hyperparameters.\n",
        "\n",
        "Please note that this metric calculates the quality score of the clustering based on distance, i.e. factors such as cluster diameter, average distance between cluster points, distance between separate clusters etc..\n",
        "Yet, as UMAP focuses on preserving the local structure of the data, the distances between clusters as well as the size of the clusters themselves are not interpretable. Moreover, some of the used clustering algorithms do not cluster according to distance but according to density, distribution, or graph structures. Therefore, the used metric does not perfectly evaluate the quality of the clustering and merely serves as an approximative assistance for the evalutation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ2DgnBXeegT"
      },
      "source": [
        "## 7.1 K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jNuYSkX0Nt9D",
        "outputId": "64c294ae-dc05-46d4-ce81-c6c57d7b1c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['adapters' 'feature_stand' 'UMAP' '2' '2' '0.4' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.3' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.6' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.3' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.7' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.9' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.3' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.7' 'manhattan']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.5' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '2' '2' '0.4' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.4' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.3' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.6' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.5' 'cosine']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.6' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.5' 'manhattan']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.6' 'manhattan']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.7' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.7' 'manhattan']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.8' 'euclidean']\n",
            " ['adapters' 'feature_stand' 'UMAP' '3' '2' '0.5' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.4' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.4' 'manhattan']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.6' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.8' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.9' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.5' 'manhattan']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '2' '2' '0.7' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '3' '2' '0.1' 'euclidean']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '3' '2' '0.2' 'manhattan']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '3' '2' '0.5' 'manhattan']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '3' '2' '0.6' 'manhattan']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '3' '2' '0.4' 'manhattan']\n",
            " ['adapters' 'norm_vecs' 'UMAP' '3' '2' '0.3' 'euclidean']]\n",
            "[[ 3.378288  -1.6519204]\n",
            " [-1.3375239 17.066078 ]\n",
            " [ 8.173094  16.256348 ]\n",
            " ...\n",
            " [15.8140135 -1.2894058]\n",
            " [16.07733    9.048892 ]\n",
            " [ 7.2523127 15.406978 ]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-6a3785b19869>:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure()\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
            "Traceback (most recent call last):\n",
            "  File \"<__array_function__ internals>\", line 177, in where\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.4098315 -1.2458988]\n",
            " [-0.19835   15.155466 ]\n",
            " [ 5.0484977 15.255997 ]\n",
            " ...\n",
            " [17.869959   1.8534476]\n",
            " [16.638546  10.022413 ]\n",
            " [ 5.547081  14.886041 ]]\n",
            "[[-1.9019655 -1.3828804]\n",
            " [ 0.7018281 13.06356  ]\n",
            " [ 4.6110716 16.978762 ]\n",
            " ...\n",
            " [15.668492  -2.1295192]\n",
            " [16.923805   9.526438 ]\n",
            " [ 5.3650784 16.781603 ]]\n",
            "[[ 5.214946    0.12763412]\n",
            " [22.352123    6.529311  ]\n",
            " [19.70649     5.9890723 ]\n",
            " ...\n",
            " [ 6.7625623   1.7476624 ]\n",
            " [-1.931388   13.321524  ]\n",
            " [ 3.0041745  13.011845  ]]\n",
            "[[ 9.590451    0.4823868 ]\n",
            " [16.041098   10.058055  ]\n",
            " [18.187893    8.43209   ]\n",
            " ...\n",
            " [ 8.312008    0.6020971 ]\n",
            " [-0.23025209 15.540795  ]\n",
            " [ 2.9037366  12.9291725 ]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
            "Traceback (most recent call last):\n",
            "  File \"<__array_function__ internals>\", line 177, in where\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10.035096    0.6314846 ]\n",
            " [17.263569   12.44225   ]\n",
            " [16.062506   11.140568  ]\n",
            " ...\n",
            " [ 8.945465   -0.33891827]\n",
            " [ 0.51793516 15.834822  ]\n",
            " [ 3.3785067  17.647436  ]]\n",
            "[[ 5.214946    0.12763412]\n",
            " [22.352123    6.529311  ]\n",
            " [19.70649     5.9890723 ]\n",
            " ...\n",
            " [ 6.7625623   1.7476624 ]\n",
            " [-1.931388   13.321524  ]\n",
            " [ 3.0041745  13.011845  ]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
            "Traceback (most recent call last):\n",
            "  File \"<__array_function__ internals>\", line 177, in where\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.9714965 10.871984 ]\n",
            " [ 6.218172   2.109453 ]\n",
            " [-8.533468  -2.4101446]\n",
            " ...\n",
            " [ 2.9432032 10.303691 ]\n",
            " [13.724179  -3.5490363]\n",
            " [-6.268562  -0.5372991]]\n",
            "[[ 9.600774   5.5791345]\n",
            " [21.47772    2.8897264]\n",
            " [18.237797   4.523575 ]\n",
            " ...\n",
            " [ 9.93182    4.086981 ]\n",
            " [ 0.5093076 11.334043 ]\n",
            " [ 2.5185466 14.410665 ]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6a3785b19869>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m#configuring plot settings, one color for each created label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'     '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpre_processing\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'     '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdim_reduction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'n_neighbors:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'     min_dist:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmin_dist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'     metric:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ch score:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'    nr clusters:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'   Kmeans'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mclustering_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_generation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpre_processing\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdim_reduction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_dims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'k_means'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         edgecolors=None, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2862\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2863\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \"\"\"\n\u001b[1;32m   1627\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m             projection_class, pkw = self._process_projection_requirements(\n\u001b[1;32m    756\u001b[0m                 *args, **kwargs)\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;31m# this call may differ for non-sep axes, e.g., polar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mfacecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.facecolor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_init_axis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;31m# This is moved out of __init__ because non-separable axes don't use it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXAxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYAxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/spines.py\u001b[0m in \u001b[0;36mregister_axis\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;31m# Clear the callback registry for this axis, or it may \"leak\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_set_scale\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_locators_and_formatters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDefault_majloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py\u001b[0m in \u001b[0;36mset_default_locators_and_formatters\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScalarFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_minor_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNullFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# update the minor locator for x and y axis based on rcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, useOffset, useMathText, useLocale)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_useOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museOffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_usetex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text.usetex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_useMathText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museMathText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderOfMagnitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mset_useMathText\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfont_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                     ufont = font_manager.findfont(\n\u001b[0m\u001b[1;32m    550\u001b[0m                         font_manager.FontProperties(\n\u001b[1;32m    551\u001b[0m                             \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"font.family\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36mfindfont\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;31m# _findfont_cached so to prevent using a stale cache entry after an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;31m# rcParam was changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         rc_params = tuple(tuple(mpl.rcParams[key]) for key in [\n\u001b[0m\u001b[1;32m   1257\u001b[0m             \u001b[0;34m\"font.serif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"font.sans-serif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"font.cursive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"font.fantasy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \"font.monospace\"])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;31m# _findfont_cached so to prevent using a stale cache entry after an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;31m# rcParam was changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         rc_params = tuple(tuple(mpl.rcParams[key]) for key in [\n\u001b[0m\u001b[1;32m   1257\u001b[0m             \u001b[0;34m\"font.serif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"font.sans-serif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"font.cursive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"font.fantasy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \"font.monospace\"])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#performing k-means clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "print(iter_array)\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  print(reduced_data)\n",
        "  for n_clusters in np.concatenate((range(2,10,1),range(10,101,5))): #iterating over number of clusters\n",
        "\n",
        "    kmeans = KMeans(init=\"random\", n_clusters=n_clusters, n_init=10, max_iter=300, random_state=42)\n",
        "    kmeans.fit(reduced_data)\n",
        "    labels = kmeans.labels_ #extracting labels of each sample\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels),2)\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    nr clusters:' + str(n_clusters) + '   Kmeans', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'k_means'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    nr clusters:' + str(n_clusters) + '   Kmeans', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'k_means'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "KJPGmUMspVWn",
        "outputId": "f31d5931-2aab-4fa9-8afe-0c00e4a5cda5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c97c3910-7880-4534-bc3a-002a1d8449d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c97c3910-7880-4534-bc3a-002a1d8449d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c97c3910-7880-4534-bc3a-002a1d8449d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c97c3910-7880-4534-bc3a-002a1d8449d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [values]\n",
              "Index: []"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTreXmaj_83i"
      },
      "source": [
        "## 7.2 Agglomerative Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPEY6grJd6BQ"
      },
      "outputs": [],
      "source": [
        "#performing Agglomerative clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for n_clusters in np.concatenate((range(2,10,1),range(10,101,5))): #iterating over number of clusters\n",
        "    agglo = AgglomerativeClustering(n_clusters = n_clusters)\n",
        "    agglo.fit(reduced_data)\n",
        "    labels =  agglo.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels),2)\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    nr clusters:' + str(n_clusters) + '\\n Agglo', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'agglo'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    nr clusters:' + str(n_clusters)+ '\\n Agglo', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'agglo'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1t9-0Qq6Myq9",
        "outputId": "896090e1-6484-4a78-a97c-a3f03af3ab03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d38f9558-35a5-43b3-854b-9b4d185596bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean100</th>\n",
              "      <td>23395.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean95</th>\n",
              "      <td>22551.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean90</th>\n",
              "      <td>21770.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean85</th>\n",
              "      <td>21021.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean80</th>\n",
              "      <td>20280.69</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d38f9558-35a5-43b3-854b-9b4d185596bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d38f9558-35a5-43b3-854b-9b4d185596bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d38f9558-35a5-43b3-854b-9b4d185596bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              values\n",
              "adaptersfeature_standUMAP220.3euclidean100  23395.33\n",
              "adaptersfeature_standUMAP220.3euclidean95   22551.37\n",
              "adaptersfeature_standUMAP220.3euclidean90   21770.59\n",
              "adaptersfeature_standUMAP220.3euclidean85   21021.86\n",
              "adaptersfeature_standUMAP220.3euclidean80   20280.69"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGAtNW3XALKE"
      },
      "source": [
        "## 7.3 Spectral Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge56iOjriWpP"
      },
      "outputs": [],
      "source": [
        "#performing Spectral clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for n_clusters in np.concatenate((range(2,10,1),range(10,101,5))): #iterating over different number of clusters\n",
        "    spectral = SpectralClustering(n_clusters = n_clusters)\n",
        "    spectral.fit(reduced_data)\n",
        "    labels =  spectral.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels),2)\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    nr clusters:' + str(n_clusters)+ '\\n Spectral', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'spectral'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    nr clusters:' + str(n_clusters)+ '\\n Spectral', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'spectral'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LQzE7wKN4mx"
      },
      "outputs": [],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHA7GrJiAWs6"
      },
      "source": [
        "## 7.4 BIRCH Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te7TimFYyA6N",
        "outputId": "1bd3d151-31c4-4fec-a5c0-1ae0de8c4519"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-6b8dee831ef4>:18: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure()\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (85). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (90). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (95). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (100). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (85). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (90). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (95). Decrease the threshold.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_birch.py:726: ConvergenceWarning: Number of subclusters found (82) by BIRCH is less than (100). Decrease the threshold.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#performing BIRCH clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for n_clusters in np.concatenate((range(2,10,1),range(10,101,5))): #iterating over number of clusters\n",
        "    birch = Birch(n_clusters = n_clusters)\n",
        "    birch.fit(reduced_data)\n",
        "    labels =  birch.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels))\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    nr clusters:' + str(n_clusters) + '\\n BIRCH', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'BIRCH'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    nr clusters:' + str(n_clusters) + '\\n Birch', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'BIRCH'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(n_clusters) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "43WtpLVuBoY2",
        "outputId": "4fd73d1a-c72e-4fd1-cf9e-76f1e6649667"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2392abad-f372-45fc-8799-6b41ece6ecad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean75</th>\n",
              "      <td>17358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean80</th>\n",
              "      <td>16966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean70</th>\n",
              "      <td>16880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean85</th>\n",
              "      <td>16532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.3euclidean90</th>\n",
              "      <td>16532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2392abad-f372-45fc-8799-6b41ece6ecad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2392abad-f372-45fc-8799-6b41ece6ecad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2392abad-f372-45fc-8799-6b41ece6ecad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           values\n",
              "adaptersfeature_standUMAP220.3euclidean75   17358\n",
              "adaptersfeature_standUMAP220.3euclidean80   16966\n",
              "adaptersfeature_standUMAP220.3euclidean70   16880\n",
              "adaptersfeature_standUMAP220.3euclidean85   16532\n",
              "adaptersfeature_standUMAP220.3euclidean90   16532"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87toLxrAbax"
      },
      "source": [
        "## 7.5 Optics Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZNfdQ3L0-ib"
      },
      "outputs": [],
      "source": [
        "#performing OPTICS clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for min_samples in range(20,100,5): #iterating over number of samples in a neighborhood for a point to be considered as a core point\n",
        "    optics = OPTICS(min_samples = min_samples)\n",
        "    optics.fit(reduced_data)\n",
        "    labels = optics.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels))\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '   -  ' + pre_processing + '  -   ' + dim_reduction + ' \\n ' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + ' \\n ch score:'+ str(ch_score) + '    min samples:' + str(min_samples) +  ' \\n Optics', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'Optics'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(min_samples) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + ' \\n ch score:' + str(ch_score)+ '    min samples:' + str(min_samples) + '\\n Optics', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'Optics'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(min_samples) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eexmvBKZBsI-",
        "outputId": "c4af38e0-d165-47ec-9aed-299937fc6d23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eda177be-3b58-4d97-a986-c6d4de4071dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.7euclidean100</th>\n",
              "      <td>1739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersnorm_vecsUMAP320.5manhattan100</th>\n",
              "      <td>1040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP320.6euclidean100</th>\n",
              "      <td>1027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP220.9euclidean100</th>\n",
              "      <td>992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adaptersfeature_standUMAP320.6manhattan100</th>\n",
              "      <td>756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eda177be-3b58-4d97-a986-c6d4de4071dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eda177be-3b58-4d97-a986-c6d4de4071dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eda177be-3b58-4d97-a986-c6d4de4071dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            values\n",
              "adaptersfeature_standUMAP220.7euclidean100    1739\n",
              "adaptersnorm_vecsUMAP320.5manhattan100        1040\n",
              "adaptersfeature_standUMAP320.6euclidean100    1027\n",
              "adaptersfeature_standUMAP220.9euclidean100     992\n",
              "adaptersfeature_standUMAP320.6manhattan100     756"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpEzuYw4AkaM"
      },
      "source": [
        "## 7.6 DBScan Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csrtURut9Ji-"
      },
      "outputs": [],
      "source": [
        "#performing DBScan clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for min_samples in range(20,100,10): #iterating over number of samples in a neighborhood for a point to be considered as a core point\n",
        "    for eps in range(1, 20, 1):\n",
        "      dbscan = DBSCAN(eps= eps/2, min_samples = min_samples)\n",
        "      dbscan.fit(reduced_data)\n",
        "      labels = dbscan.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels),2)\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    min samples:' + str(min_samples) + '    eps:'+ str(eps) + '\\n DBScan', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'DBScan'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(min_samples) + str(eps) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    min samples:' + str(min_samples)+ '    eps:'+ str(eps) +'\\n DBScan', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'DBScan'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(min_samples) + str(eps) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDY3KVGLBt9Y"
      },
      "outputs": [],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK7wGdw6OLPw"
      },
      "source": [
        "## 7.6 Gaussian Mixture Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qexMdvvOKbs"
      },
      "outputs": [],
      "source": [
        "#performing GMM clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for nr_components in np.concatenate((range(2,10,1),range(10,101,5))): #iterating over number of samples in a neighborhood for a point to be considered as a core point\n",
        "      gmm = mixture.GMM(nr_components=nr_components)\n",
        "      gmm.fit(reduced_data)\n",
        "      labels = gmm.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels),2)\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    min samples:' + str(min_samples) + '\\n DBScan', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'gmm'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(nr_components) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    min samples:' + str(min_samples)+ '\\n DBScan', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'gmm'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(nr_components) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXZ0lDYwPyWn"
      },
      "outputs": [],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB1vN5JxPq4S"
      },
      "source": [
        "##7.7 Bayesian Gaussian Mixture Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGZWBcgBPwY0"
      },
      "outputs": [],
      "source": [
        "#performing GMM clustering by iterating over plausible hyperparameter values\n",
        "\n",
        "#creating dict to track ch scores\n",
        "ch_tracker = {}\n",
        "\n",
        "#iterating over the different hyperparameter values and reduced dimensionality embeddings\n",
        "for data_generation, pre_processing, dim_reduction, n_dims, n_neighbors, min_dist, measure in iter_array: #iteration over different reduced dim embeddings\n",
        "  reduced_data = embeddings[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure]\n",
        "  for nr_components in np.concatenate((range(2,10,1),range(10,101,5))): #iterating over number of samples in a neighborhood for a point to be considered as a core point\n",
        "      vgmm = BayesianGaussianMixture(n_components=nr_components, random_state=42)\n",
        "      vgmm.fit(reduced_data)\n",
        "      labels = vgmm.labels_\n",
        "    ch_score = round(metrics.calinski_harabasz_score(reduced_data, labels),2)\n",
        "    ch_tracker[data_generation + pre_processing + dim_reduction + n_dims + n_neighbors+ min_dist + measure + str(n_clusters)] = ch_score\n",
        "\n",
        "    if n_dims == '2':\n",
        "      #configuring plot settings, one color for each created label\n",
        "      plt.figure()\n",
        "      plt.scatter(reduced_data[:,0], reduced_data[:,1], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score) + '    min samples:' + str(min_samples) + '\\n DBScan', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'vgmm'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(nr_components) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')\n",
        "\n",
        "\n",
        "    else:\n",
        "      plt.figure()\n",
        "      ax = plt.axes(projection='3d')\n",
        "      ax.scatter3D(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c = np.take(colors, labels), s=0.1)\n",
        "      plt.title(data_generation + '     ' + pre_processing + '     ' + dim_reduction + '\\n' + 'n_neighbors:' + n_neighbors+ '     min_dist:' + min_dist + '     metric:' + measure + '\\n' + 'ch score:'+str(ch_score)+ '    min samples:' + str(min_samples)+ '\\n DBScan', fontsize=8)\n",
        "      clustering_path = w_path + '/' + data_generation + '/' + pre_processing + '/' + dim_reduction + '/' + n_dims + '/' + 'vgmm'\n",
        "      if os.path.exists(clustering_path) == False:\n",
        "        os.mkdir(clustering_path)\n",
        "      clustering_path = clustering_path + '/' + n_neighbors + '_' + min_dist + '_' + measure + str(nr_components) +'.pdf'\n",
        "      plt.savefig(clustering_path , bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7fG1f2wPzUX"
      },
      "outputs": [],
      "source": [
        "#Determining the best clusters according to ch score and printing them in table\n",
        "largest_keys = sorted(ch_tracker, key=ch_tracker.get, reverse=True)[:5]\n",
        "largest_vals = [ch_tracker[x] for x in largest_keys]\n",
        "length = len(largest_vals)\n",
        "heading = np.empty(length, dtype = str)\n",
        "heading[:] = 'value'\n",
        "pd.DataFrame(largest_vals, index = largest_keys, columns=[\"values\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1N3LUQdGg0U"
      },
      "source": [
        "# 8. Testing Cluster Validity\n",
        "In this section, we will validate our clustering.\n",
        "\n",
        "We will proceed by checking the interpretability of the different clusters. In order to do so, we will sample user embeddings from the different resulting clusters. In a next step, we will transform them back to their original dimensionality to be able to pass them through our recommender system. The recommender system will then yield the representative word clouds for each cluster.\n",
        "\n",
        "If the word clouds from different clusters vary significantly from one another and provide a basis for proper interpretability, the clustering will be deemed valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuX14xMAIFOJ"
      },
      "outputs": [],
      "source": [
        "#1. Perform UMAP with chosen hyperparams\n",
        "#2. Perform Clustering with chosen hyperparams\n",
        "#3. Sample 5 users from each cluster\n",
        "#4. Backproject samples to original embedding dimensionality\n",
        "#5. Send embeddings through recommender systems in cluster-batches to withdraw key words for each cluster\n",
        "#6. Build word cloud from key-words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "92wlMVhTK-I7",
        "Hl1ZDb53R0S-",
        "oz6p3uN3MSTw",
        "YZ2DgnBXeegT",
        "aTreXmaj_83i",
        "EGAtNW3XALKE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}